/**
 * Multi AI Provider Abstraction Layer
 * Gemini, Groq, Mistral, OpenRouter í†µí•©
 * í´ë°± ìˆœì„œ: Gemini â†’ Groq â†’ Mistral â†’ OpenRouter â†’ Mock
 */

require('dotenv').config();

const SYSTEM_PROMPT = 'ë‹¹ì‹ ì€ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ 100% ìˆœìˆ˜ í•œê¸€ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¤‘êµ­ì–´(æ¼¢å­—), ì¼ë³¸ì–´, ì˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.';

// AI Provider ì„¤ì • (ìš°ì„ ìˆœìœ„: Gemini > Groq > Mistral > OpenRouter)
const AI_PROVIDERS = [
  {
    name: 'gemini',
    model: 'gemini-2.0-flash',
    priority: 1,
    enabled: () => !!process.env.GEMINI_API_KEY
  },
  {
    name: 'groq',
    model: 'llama-3.3-70b-versatile',
    endpoint: 'https://api.groq.com/openai/v1/chat/completions',
    priority: 2,
    enabled: () => !!process.env.GROQ_API_KEY
  },
  {
    name: 'mistral',
    model: 'mistral-small-latest',
    endpoint: 'https://api.mistral.ai/v1/chat/completions',
    priority: 3,
    enabled: () => !!process.env.MISTRAL_API_KEY
  },
  {
    name: 'openrouter',
    model: 'meta-llama/llama-3.3-70b-instruct:free',
    endpoint: 'https://openrouter.ai/api/v1/chat/completions',
    priority: 4,
    enabled: () => !!process.env.OPENROUTER_API_KEY
  }
];

// Gemini ì´ˆê¸°í™” (lazy loading)
let geminiModel = null;

function getGeminiModel() {
  if (!geminiModel && process.env.GEMINI_API_KEY) {
    try {
      const { GoogleGenerativeAI } = require('@google/generative-ai');
      const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
      geminiModel = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });
      console.log('âœ… Gemini model initialized (gemini-2.0-flash)');
    } catch (err) {
      console.error('âŒ Gemini initialization failed:', err.message);
    }
  }
  return geminiModel;
}

/**
 * Gemini API í˜¸ì¶œ (ë¶„ë‹¹ ì¿¼í„° ì´ˆê³¼ ì‹œ ìë™ ì¬ì‹œë„)
 */
async function callGemini(prompt, retries = 2) {
  const model = getGeminiModel();
  if (!model) {
    throw new Error('Gemini model not initialized');
  }

  for (let attempt = 0; attempt <= retries; attempt++) {
    try {
      const result = await model.generateContent(prompt);
      const response = await result.response;
      return response.text();
    } catch (err) {
      const is429 = err.message && (err.message.includes('429') || err.message.includes('quota') || err.message.includes('RESOURCE_EXHAUSTED'));
      if (is429 && attempt < retries) {
        const waitSec = 10 * (attempt + 1);
        console.log(`â³ Gemini ë¶„ë‹¹ ì¿¼í„° ì´ˆê³¼, ${waitSec}ì´ˆ í›„ ì¬ì‹œë„ (${attempt + 1}/${retries})...`);
        await new Promise(r => setTimeout(r, waitSec * 1000));
        continue;
      }
      throw err;
    }
  }
}

/**
 * OpenAI í˜¸í™˜ API í˜¸ì¶œ (Groq, Mistral, OpenRouter ê³µí†µ)
 */
async function callOpenAICompatible(provider, prompt) {
  const config = AI_PROVIDERS.find(p => p.name === provider);
  if (!config) throw new Error(`Unknown provider: ${provider}`);

  const apiKeyMap = {
    'groq': process.env.GROQ_API_KEY,
    'mistral': process.env.MISTRAL_API_KEY,
    'openrouter': process.env.OPENROUTER_API_KEY
  };

  const headers = {
    'Authorization': `Bearer ${apiKeyMap[provider]}`,
    'Content-Type': 'application/json'
  };

  // OpenRouterëŠ” ì¶”ê°€ í—¤ë” í•„ìš”
  if (provider === 'openrouter') {
    headers['HTTP-Referer'] = 'https://travel-pms.onrender.com';
    headers['X-Title'] = 'TravelPMS';
  }

  const response = await fetch(config.endpoint, {
    method: 'POST',
    headers,
    body: JSON.stringify({
      model: config.model,
      messages: [
        { role: 'system', content: SYSTEM_PROMPT },
        { role: 'user', content: prompt }
      ],
      temperature: 0.7,
      max_tokens: 2000
    })
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`${provider} API error: ${response.status} - ${error}`);
  }

  const data = await response.json();
  return data.choices[0].message.content;
}

/**
 * í†µí•© AI í˜¸ì¶œ í•¨ìˆ˜ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜ í´ë°±)
 * Gemini â†’ Groq â†’ Mistral â†’ OpenRouter â†’ Mock
 */
async function generateAIResponse(prompt) {
  const enabledProviders = AI_PROVIDERS
    .filter(p => p.enabled())
    .sort((a, b) => a.priority - b.priority);

  if (enabledProviders.length === 0) {
    console.warn('âš ï¸ No AI providers enabled, use Mock mode');
    return { response: null, provider: 'mock' };
  }

  for (const provider of enabledProviders) {
    try {
      console.log(`ğŸ¤– Trying ${provider.name} (${provider.model})...`);

      let response;
      if (provider.name === 'gemini') {
        response = await callGemini(prompt);
      } else {
        response = await callOpenAICompatible(provider.name, prompt);
      }

      console.log(`âœ… ${provider.name} response received`);
      return { response, provider: provider.name };

    } catch (err) {
      console.warn(`âš ï¸ ${provider.name} failed: ${err.message}`);

      const isQuotaError = err.message && (
        err.message.includes('quota') ||
        err.message.includes('429') ||
        err.message.includes('Too Many Requests') ||
        err.message.includes('rate limit')
      );

      if (isQuotaError) {
        console.error(`âŒ ${provider.name} quota exceeded, trying next provider...`);
      }

      continue;
    }
  }

  console.error('âŒ All AI providers failed, fallback to Mock mode');
  return { response: null, provider: 'mock' };
}

/**
 * í˜„ì¬ í™œì„±í™”ëœ Provider ì •ë³´
 */
function getProviderStatus() {
  return AI_PROVIDERS.map(p => ({
    name: p.name,
    model: p.model,
    enabled: p.enabled(),
    priority: p.priority
  }));
}

/**
 * í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ Provider ì´ë¦„
 */
function getActiveProvider() {
  const enabled = AI_PROVIDERS.filter(p => p.enabled());
  if (enabled.length === 0) return 'mock';
  return enabled.sort((a, b) => a.priority - b.priority)[0].name;
}

module.exports = {
  generateAIResponse,
  getProviderStatus,
  getActiveProvider,
  AI_PROVIDERS
};
