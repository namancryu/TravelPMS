/**
 * Multi AI Provider Abstraction Layer
 * Gemini, Grok(xAI), Groq, Together.ai í†µí•©
 */

require('dotenv').config();

// AI Provider ì„¤ì • (ìš°ì„ ìˆœìœ„: Gemini > Grok > Groq > Together)
const AI_PROVIDERS = [
  {
    name: 'gemini',
    model: 'gemini-2.0-flash',
    priority: 1,
    enabled: () => !!process.env.GEMINI_API_KEY
  },
  {
    name: 'grok',
    model: 'grok-3-mini',
    endpoint: 'https://api.x.ai/v1/chat/completions',
    priority: 2,
    enabled: () => !!process.env.XAI_API_KEY
  },
  {
    name: 'groq',
    model: 'llama-3.3-70b-versatile',
    endpoint: 'https://api.groq.com/openai/v1/chat/completions',
    priority: 3,
    enabled: () => !!process.env.GROQ_API_KEY
  },
  {
    name: 'together',
    model: 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',
    endpoint: 'https://api.together.xyz/v1/chat/completions',
    priority: 4,
    enabled: () => !!process.env.TOGETHER_API_KEY
  }
];

// Gemini ì´ˆê¸°í™” (lazy loading)
let geminiModel = null;

function getGeminiModel() {
  if (!geminiModel && process.env.GEMINI_API_KEY) {
    try {
      const { GoogleGenerativeAI } = require('@google/generative-ai');
      const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
      geminiModel = genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });
      console.log('âœ… Gemini model initialized (gemini-2.0-flash)');
    } catch (err) {
      console.error('âŒ Gemini initialization failed:', err.message);
    }
  }
  return geminiModel;
}

/**
 * Gemini API í˜¸ì¶œ (ë¶„ë‹¹ ì¿¼í„° ì´ˆê³¼ ì‹œ ìë™ ì¬ì‹œë„)
 */
async function callGemini(prompt, retries = 2) {
  const model = getGeminiModel();
  if (!model) {
    throw new Error('Gemini model not initialized');
  }

  for (let attempt = 0; attempt <= retries; attempt++) {
    try {
      const result = await model.generateContent(prompt);
      const response = await result.response;
      return response.text();
    } catch (err) {
      const is429 = err.message && (err.message.includes('429') || err.message.includes('quota') || err.message.includes('RESOURCE_EXHAUSTED'));
      if (is429 && attempt < retries) {
        const waitSec = 10 * (attempt + 1);
        console.log(`â³ Gemini ë¶„ë‹¹ ì¿¼í„° ì´ˆê³¼, ${waitSec}ì´ˆ í›„ ì¬ì‹œë„ (${attempt + 1}/${retries})...`);
        await new Promise(r => setTimeout(r, waitSec * 1000));
        continue;
      }
      throw err;
    }
  }
}

/**
 * Grok (xAI) API í˜¸ì¶œ (OpenAI í˜¸í™˜ í˜•ì‹)
 */
async function callGrok(prompt) {
  const response = await fetch('https://api.x.ai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.XAI_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'grok-3-mini',
      messages: [
        { role: 'system', content: 'ë‹¹ì‹ ì€ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ 100% ìˆœìˆ˜ í•œê¸€ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¤‘êµ­ì–´(æ¼¢å­—), ì¼ë³¸ì–´, ì˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.' },
        { role: 'user', content: prompt }
      ],
      temperature: 0.7,
      max_tokens: 2000
    })
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Grok API error: ${response.status} - ${error}`);
  }

  const data = await response.json();
  return data.choices[0].message.content;
}

/**
 * Groq API í˜¸ì¶œ (OpenAI í˜¸í™˜ í˜•ì‹)
 */
async function callGroq(prompt) {
  const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.GROQ_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'llama-3.3-70b-versatile',
      messages: [
        { role: 'system', content: 'ë‹¹ì‹ ì€ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ 100% ìˆœìˆ˜ í•œê¸€ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¤‘êµ­ì–´(æ¼¢å­—), ì¼ë³¸ì–´, ì˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.' },
        { role: 'user', content: prompt }
      ],
      temperature: 0.7,
      max_tokens: 2000
    })
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Groq API error: ${response.status} - ${error}`);
  }

  const data = await response.json();
  return data.choices[0].message.content;
}

/**
 * Together.ai API í˜¸ì¶œ (OpenAI í˜¸í™˜ í˜•ì‹)
 */
async function callTogether(prompt) {
  const response = await fetch('https://api.together.xyz/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.TOGETHER_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',
      messages: [
        { role: 'system', content: 'ë‹¹ì‹ ì€ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ 100% ìˆœìˆ˜ í•œê¸€ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¤‘êµ­ì–´(æ¼¢å­—), ì¼ë³¸ì–´, ì˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.' },
        { role: 'user', content: prompt }
      ],
      temperature: 0.7,
      max_tokens: 2000
    })
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Together API error: ${response.status} - ${error}`);
  }

  const data = await response.json();
  return data.choices[0].message.content;
}

/**
 * í†µí•© AI í˜¸ì¶œ í•¨ìˆ˜ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜ í´ë°±)
 * Gemini â†’ Grok â†’ Groq â†’ Together â†’ Mock
 */
async function generateAIResponse(prompt) {
  const enabledProviders = AI_PROVIDERS
    .filter(p => p.enabled())
    .sort((a, b) => a.priority - b.priority);

  if (enabledProviders.length === 0) {
    console.warn('âš ï¸ No AI providers enabled, use Mock mode');
    return { response: null, provider: 'mock' };
  }

  for (const provider of enabledProviders) {
    try {
      console.log(`ğŸ¤– Trying ${provider.name} (${provider.model})...`);

      let response;
      switch (provider.name) {
        case 'gemini':
          response = await callGemini(prompt);
          break;
        case 'grok':
          response = await callGrok(prompt);
          break;
        case 'groq':
          response = await callGroq(prompt);
          break;
        case 'together':
          response = await callTogether(prompt);
          break;
        default:
          continue;
      }

      console.log(`âœ… ${provider.name} response received`);
      return { response, provider: provider.name };

    } catch (err) {
      console.warn(`âš ï¸ ${provider.name} failed: ${err.message}`);

      const isQuotaError = err.message && (
        err.message.includes('quota') ||
        err.message.includes('429') ||
        err.message.includes('Too Many Requests') ||
        err.message.includes('rate limit')
      );

      if (isQuotaError) {
        console.error(`âŒ ${provider.name} quota exceeded, trying next provider...`);
      }

      continue;
    }
  }

  console.error('âŒ All AI providers failed, fallback to Mock mode');
  return { response: null, provider: 'mock' };
}

/**
 * í˜„ì¬ í™œì„±í™”ëœ Provider ì •ë³´
 */
function getProviderStatus() {
  return AI_PROVIDERS.map(p => ({
    name: p.name,
    model: p.model,
    enabled: p.enabled(),
    priority: p.priority
  }));
}

/**
 * í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ Provider ì´ë¦„
 */
function getActiveProvider() {
  const enabled = AI_PROVIDERS.filter(p => p.enabled());
  if (enabled.length === 0) return 'mock';
  return enabled.sort((a, b) => a.priority - b.priority)[0].name;
}

module.exports = {
  generateAIResponse,
  getProviderStatus,
  getActiveProvider,
  AI_PROVIDERS
};
